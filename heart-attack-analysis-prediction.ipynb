{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings  \nwarnings.filterwarnings('ignore')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/heart-attack-analysis-prediction-dataset/heart.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Basic EDA**","metadata":{}},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,8))\nsns.heatmap(data.corr(),annot=True,cmap=\"Oranges\",ax=ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data[\"output\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data is well balanced","metadata":{}},{"cell_type":"code","source":"sns.countplot(data[\"sex\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this We know that number of men is more than number of female","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,8))\ndata.boxplot(ax=ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are some outliers but i have decided to keep them cause logically i think they are the reason behind the heart attck.","metadata":{}},{"cell_type":"markdown","source":"**Model**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import  BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var_col = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\ndata1 = pd.get_dummies(data,columns=['sex',\"cp\",\"fbs\",\"restecg\",\"exng\",\"slp\",\"thall\"],drop_first=True )\n\nscale = StandardScaler()\ndata1[var_col] = scale.fit_transform(data1[var_col])\nX = data1.drop(\"output\",axis = 1)\ny = data1[[\"output\"]]\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state=1001)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_ada = {'base_estimator__max_depth':[i for i in range(2,11,2)],\n              'base_estimator__min_samples_leaf':[5,10],\n              'n_estimators':[10,50,250,1000],\n              'learning_rate':[0.01,0.1]}\nada= AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\nadaGSCV = GridSearchCV(ada, param_grid=params_ada,verbose=3,scoring='f1',n_jobs=-1)\nbagc = BaggingClassifier(base_estimator=DecisionTreeClassifier())\n\nparams_bagc = {'base_estimator__max_depth':[i for i in range(2,11,2)],\n              'base_estimator__min_samples_leaf':[5,10],\n              'n_estimators':[10,50,250,1000]}\nbagcGSCV=  GridSearchCV(bagc, param_grid=params_bagc,verbose=3,scoring='f1',n_jobs=-1)            \nmodel_list1 = [(\"ada\",adaGSCV),(\"bagging\",bagcGSCV)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for modelname, model in model_list1:\n  model.fit(X_train,y_train)\n  model.predict(X_test)\n  best_para = model.best_params_\n  score_m = model.score(X_test,y_test)\n  print(modelname, \":\", score_m)\n  print(modelname, \":\", best_para)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I decided to use XGBClassifier after this ","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import *\nxgb_model = XGBClassifier()\nparameters_xgb = {'objective':['binary:logistic',\"binary:hinge\",],\n                  'learning_rate': [0.01,0.1], #so called `eta` value\n                  'max_depth': [i for i in range(2,11,2)],\n                  'n_estimators': [510,50,250,1000]}\nclf = GridSearchCV(xgb_model, parameters_xgb, n_jobs=5,\n                   scoring='roc_auc',\n                   verbose=3, refit=True)\nclf.fit(X_train,y_train)\nprint(clf.score(X_test,y_test))\nprint(clf.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using XGBClassifier really helped.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nprediction_XGB = clf.predict(X_test)\nconfusion_matrix_XGB = confusion_matrix(y_test,prediction_XGB) \nsns.heatmap(confusion_matrix_XGB,annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, clf.predict(X_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}